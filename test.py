from __future__ import annotations
st.metric("ì´ ê¶Œìˆ˜", len(edited))
with stats2:
st.metric("ê³ ìœ  ì €ì ìˆ˜", edited["authors"].fillna("").apply(lambda s: [a.strip() for a in s.split(",") if a.strip()] ).explode().nunique())
with stats3:
pages = pd.to_numeric(edited["pageCount"], errors="coerce").dropna()
st.metric("ì´ í˜ì´ì§€", int(pages.sum()) if len(pages) else 0)
with stats4:
this_year = str(datetime.today().year)
st.metric("ì˜¬í•´ ì½ì€ ì±…", int((edited["date_read"].fillna("").str.startswith(this_year)).sum()))


st.markdown("---")


# -----------------------------
# ë¶„ì„ ëŒ€ì‹œë³´ë“œ
# -----------------------------
st.subheader("ğŸ“ˆ ë¶„ì„ ëŒ€ì‹œë³´ë“œ")


if st.session_state.library.empty:
st.info("ë°ì´í„°ê°€ ìˆì–´ì•¼ ë¶„ì„í•  ìˆ˜ ìˆì–´ìš”. ìœ„ì—ì„œ ì±…ì„ ì¶”ê°€í•´ ì£¼ì„¸ìš”.")
else:
df = st.session_state.library.copy()
df["date_read_parsed"] = pd.to_datetime(df["date_read"], errors="coerce")


st.markdown("#### ğŸ“Š ì›”ë³„ ë…ì„œëŸ‰ ì¶”ì´")
only_12 = st.toggle("ìµœê·¼ 12ê°œì›”ë§Œ ë³´ê¸°", value=True)
ts = (
df.dropna(subset=["date_read_parsed"])
.assign(month=lambda x: x["date_read_parsed"].dt.to_period("M").dt.to_timestamp())
.groupby("month").size().rename("count").reset_index()
.sort_values("month")
)
if only_12 and not ts.empty:
cutoff = datetime.today() - relativedelta(months=11)
ts = ts[ts["month"] >= cutoff.replace(day=1)]


if ts.empty:
st.write(":grey[í‘œì‹œí•  ë°ì´í„°ê°€ ì—†ì–´ìš”.]")
else:
fig1, ax1 = plt.subplots()
ax1.plot(ts["month"], ts["count"], marker="o")
ax1.set_xlabel("ì›”")
ax1.set_ylabel("ê¶Œìˆ˜")
ax1.set_title("ì›”ë³„ ì½ì€ ê¶Œìˆ˜")
ax1.grid(True, linestyle=":", alpha=0.4)
st.pyplot(fig1, use_container_width=True)


st.markdown("---")


st.markdown("#### â˜ï¸ ì¥ë¥´/ì¹´í…Œê³ ë¦¬ ì›Œë“œí´ë¼ìš°ë“œ")
cats = df["categories"].dropna().astype(str)
tokens: List[str] = []
for c in cats:
parts = [p.strip() for p in c.replace("/", ",").split(",") if p.strip()]
tokens.extend(parts)
if not tokens:
st.write(":grey[ì¥ë¥´ ì •ë³´ê°€ ì—†ì–´ ì›Œë“œí´ë¼ìš°ë“œë¥¼ ë§Œë“¤ ìˆ˜ ì—†ì–´ìš”.]")
else:
freq = pd.Series(tokens).value_counts().to_dict()
wc = WordCloud(width=1000, height=500, background_color="white")
wc.generate_from_frequencies(freq)
fig2, ax2 = plt.subplots(figsize=(10,5))
ax2.imshow(wc, interpolation="bilinear")
ax2.axis("off")
st.pyplot(fig2, use_container_width=True)


st.markdown("---")


st.markdown("#### ğŸ·ï¸ ê°€ì¥ ë§ì´ ì½ì€ ì €ì/ì¶œíŒì‚¬")
top_n = st.slider("TOP N", 3, 15, 5, key="topn")


c1, c2 = st.columns(2)
with c1:
    st.markdown("**ì €ì TOP**")
    if "Author" in df.columns:
        top_authors = (
            df["Author"]
            .dropna()
            .str.split(",")
            .explode()
            .str.strip()
            .value_counts()
            .head(10)
        )
        st.bar_chart(top_authors)

with c2:
    st.markdown("**ì¶œíŒì‚¬ TOP**")
    if "Publisher" in df.columns:
        top_publishers = (
            df["Publisher"]
            .dropna()
            .str.split(",")
            .explode()
            .str.strip()
            .value_counts()
            .head(10)
        )
        st.bar_chart(top_publishers)

